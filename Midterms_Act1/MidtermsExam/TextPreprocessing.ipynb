{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2451b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/louezetheianilicirsaldua/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/louezetheianilicirsaldua/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/louezetheianilicirsaldua/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/louezetheianilicirsaldua/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import inflect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import requests\n",
    "import io\n",
    "import csv\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "976704e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for text cleaning and preprocessing\n",
    "def importdata(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    raw_text = response.text\n",
    "\n",
    "    # Requote rows\n",
    "    rows = []\n",
    "    reader = csv.reader(io.StringIO(raw_text), delimiter=',', quotechar='\"', skipinitialspace=True)\n",
    "    max_cols = 0\n",
    "    for row in reader:\n",
    "        if len(row) > max_cols:\n",
    "            max_cols = len(row)\n",
    "        rows.append(row)\n",
    "\n",
    "    # Pad rows so all have equal length\n",
    "    for row in rows:\n",
    "        if len(row) < max_cols:\n",
    "            row += [None] * (max_cols - len(row))\n",
    "\n",
    "    df = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "    print(f\"Loaded DataFrame with {len(df)} rows and {len(df.columns)} columns.\")\n",
    "    return df\n",
    "\n",
    "    return raw_text\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = text.lower()                 # Force lowercase\n",
    "    text = re.sub(r'\\d+', '', text)     # Remove numbers\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)   # Remove punctuation\n",
    "    text = \" \".join(text.split())       # Rem whitespace\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    text = [word for word in word_tokens if word.lower()\n",
    "                     not in stop_words] # Rem stop words\n",
    "\n",
    "    word_tokens = word_tokenize(text)   # Tokenize\n",
    "    text = [stemmer.stem(word) for word in word_tokens]\n",
    "\n",
    "    word_tokens = word_tokenize(text)   # Lemmatization\n",
    "    text = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "\n",
    "    # Create new column for categorized job positions\n",
    "    text[\"categorized_title\"] = text[\"title\"]\n",
    "    \n",
    "def populate_categ_title(title):\n",
    "    doc = nlp(title)\n",
    "    nouns = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN']]\n",
    "    if not nouns:\n",
    "        return 'other'\n",
    "    # Return most frequent noun\n",
    "    return Counter(nouns).most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87fde77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DataFrame with 790 rows and 5 columns.\n",
      "Index(['',\n",
      "       'title                                                                   ',\n",
      "       'company                                                   ',\n",
      "       'announcement                                               ',\n",
      "       'description'],\n",
      "      dtype='object')\n",
      "     title                                                                     \\\n",
      "0  0  Senior Analyst, Data Science and Analytics    ...                         \n",
      "1  1  Senior Data Scientist                         ...                         \n",
      "2  2  Lead Data Science Analyst                     ...                         \n",
      "3  3  Data Science Intern                           ...                         \n",
      "4  4  Data Scientist                                ...                         \n",
      "\n",
      "  company                                                     \\\n",
      "0  TransUnion                                    ...           \n",
      "1  Grubhub Holdings, Inc.                        ...           \n",
      "2  Discover Financial Services                   ...           \n",
      "3  AbelsonTaylor                                 ...           \n",
      "4  NORC at the University of Chicago             ...           \n",
      "\n",
      "  announcement                                                 \\\n",
      "0  The Muse                                      ...            \n",
      "1  ZipRecruiter                                  ...            \n",
      "2  LinkedIn                                      ...            \n",
      "3  Startup Jobs                                  ...            \n",
      "4  SimplyHired                                   ...            \n",
      "\n",
      "                                         description  \n",
      "0  TransUnion's Job Applicant Privacy Notice  Wha...  \n",
      "1  About The Opportunity  We're all about connect...  \n",
      "2  Discover. A brighter future.  With us, youâ€™ll ...  \n",
      "3  Are you a 2023 college graduate or rising coll...  \n",
      "4  JOB DESCRIPTION:  At NORC, Data Scientists pla...  \n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/MaharLeika18/Data-Mining---Python/refs/heads/Loue/Midterms_Act1/MidtermsExam/Jobs.csv\"\n",
    "df = importdata(url)\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = text_cleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categorized_title'] = df['title'].apply(populate_categ_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
